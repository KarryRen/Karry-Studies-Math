# 概率论与数理统计

<center>Karry Ren</center>

> 在我看来，概率论与数理统计是最重要的一门数学课，是当下金融学、人工智能的本质基础，是刻画世界的最有力的工具。因此我的后半生一定会一直和其打交道，考虑到之前的学习过程比较分散，未形成自己的体系。从 2023 年 8 月开始整理此笔记，目的就是打造一则最完善的概率论与数理统计基础宝典，再次疏通整个概率论数理统计框架，掌握最基础最深层次的含义，并用自己的话加以描述，为日后的使用奠定坚实的基础。
>
> 本笔记参考茆诗松老师的教材整理而来，常见题目出自考研课程。

[toc]

## 第一章 随机事件与概率

### 1.1 随机事件及其运算

所有的数学都一定是从看得见摸得着的事物出发的，概率同样是如此，概率的直观概念就是随机事件发生的可能性大小。因此我们先研究生活中可能看到的一些随机事件，并用数学语言进行刻画，并进行相应的集合运算（交、并、补、差、互斥<一分为多>、对立<一分为二>等等），得到相关的运算法则（德摩根定率）。
$$
	\overline{A \cup B} = \overline{A} \cap \overline{B}\\
	\overline{A \cap B} = \overline{A} \cup \overline{B}
$$
刻画随机事件（连续 or 离散）让我们对概率所依附的实体有了直观的认识。同时也要要理解清楚样本空间的含义是什么。

==【定义】样本空间 and 样本点==：随机现象的一切可能基本结果组成的集合成为**样本空间**，记为 $\Omega = \{\omega\}$ ，其中 $\omega$ 表示基本结果，又称为**样本点**。样本点就是概率统计中抽样、计算的基本单元，认识随机现象首先要列出他的样本空间。

### 1.2 概率的定义及其确定方法

#### 1.2.1 概率的公理化定义

> 尽管从直观上出发，通过解释随机事件，能够给出“概率”的定义。
>
> 但是无论是哪种定义（古典、几何）方式都无法适应于一切随机现象。因此：
>
> - 1900 年数学家 Hilbert 提出要建立概率的公理化定义，以最少的几条本质特性来刻画概率的概念。
> - 1933 年 Kolmogorov 提出了如下概率的公理化定义。

设 $\Omega$ 为一个样本空间，$\mathscr{F}$ 为 $\Omega$ 的某些子集组成的一个**事件域**（简单来说就是样本空间中各样本点的各种组合，因此事件域肯定包含样本空间），如果对任一事件 $A \in \mathscr{F}$，定义在 $\mathscr{F}$ 上的一个**实值函数** $P(A)$ 满足：

- 非负性公理：若 $A \in \mathscr{F}$，则 $P(A) \ge 0$；

- 正则性公理：$P (\Omega) = 1$；

- 可列可加性公理：若 $A_1, A_2, \dots, A_n, \dots$ 互不相容，有（和的概率等于概率之和）
  $$
  P \biggl( \bigcup _{i=1} ^{+\infty} A_i \biggr) = \sum _{i=1} ^{+\infty} P ( A _i ),
  $$

则称 $P (A)$ 为事件 $A$ 的概率，称三元素 $(\Omega, \mathscr{F}, P)$ 为**概率空间**。

```c++
可列可加性是推出一系列概率公式的出发点 P(A + B); P(A - B) ... 
```

#### 1.2.2 概率的确定方法

> 确定了概率是什么，还需要知道概率如何计算。其实本质上来讲概率的计算可以划分为两类：
>
> - 离散型随机变量的概率计算（古典方法）
> - 连续领随机变量的概率计算（几何方法）

##### a. 确定概率的古典方法

对于在离散的样本空间中的**<u>有限个样本点且等可能</u>**而言，可以直接通过**排列组合**的方式直接计算（本质就是数点）。常见的模型有：抽样模型（放回、不放回）以及盒子模型等。

对于这类问题，计算最复杂但思维清晰的思路是（有些多情况可以不做唯一标识进行简化计算，但是思维的复杂度会增加）：

- 对样本空间内的每种情况做唯一标识，做分母
- 对符合条件的每种情况做唯一标识，做分子

==古典方法的例子==

- 盒子模型

  ```c++
  设有 n 个球，每个球都等可能地被放到 N 个不同的盒子中的任意一个，每个盒子所放球数不限，求：
  （1） 指定的 n （n <= N）个盒子中各有一球的概率 p1
  （2） 恰好有 n （n <= N）个盒子中各有一球的概率 p2
  
  // 思路：对于这个题目而言，我们既可以把所有的球看作相通的，也可以给每个球编号看作不同的。
  // 为了简化思维，最好的方式就是给每个球编号进而对样本空间的每种情况做唯一标识。
  （1）  
  		- 分母（全样本空间）：n 个球，每个球都有 N 中放法 => n^N；
  		- 分子（满足情况的点）：把 n 个球放到 n 个盒子中，而且是互斥的 => n!
  		- 结果：n! / n^N
  （2）
  		- 分母（全样本空间）不变。
  		- 分子（满足情况的点）：多了一步在 N 中选 n 个盒子的步骤，所以可能性就多了 * C(Nn)
  		- 结果：上述结果 * C(Nn)
  ```

##### b. 确定概率的几何方法

对于在连续的样本空间中的无限个样本点而言，无法通过直接数点的方式进行计算。常见的例子有：会面问题、比丰投针问题（算 $\pi$）、算函数的积分（**<u>*蒙特卡洛模拟*</u>**）等。

对于这类问题，思路和上述本质是一样的，不过分子分母从之前的点的数量变成了一维（线的长度），二维（面积），三维（提体积）之比。

==几何方法的例子==

- [贝特朗奇论](https://blog.csdn.net/qq_44872466/article/details/116205252)

  ```c++
  在一圆内任取一条弦，问其长度超过该圆内接等边三角形的边长的概率是多少。
  针对这一个问题，有三种不同的解题思路得到三种完全不同的答案。（所占的整体立场不同，结果就会截然不同）
  ```

### 1.3 概率的性质

#### 1.3.1 概率的基本性质

由概率的公理化定义可以轻松得到概率的如下性质：有限可加性、单调性。**<u>条件概率满足一切的基本概率性质。</u>**在这我们强调两个公式，因为经常用到

- **加法公式（三个事件）**
  $$
  P(A + B + C) = (P(A) + P(B) + P(C)) - (P(AB) + P(AC) + P(BC)) + P(ABC)
  $$

- **互斥公式**
  $$
  P(AB\overline C) = P(AB) - P(ABC)
  $$

#### 1.3.2 条件概率及其性质 

##### a. 条件概率的定义

$$
P(B|A) = \frac{P(AB)}{P(A)}
$$

##### b. 全概率公式

$$
P(B) = \sum P(A_i)P(B|A_i)
$$

全概率公式是多个分散情况到集中情况的汇总：是在已经知道多个简单事件（构成完备事件组）的概率，以及在每个简单事件发生情况下复杂情事件发生的条件概率的情况下，求解复杂事件所发生的实际概率（积沙成塔，条条大路通罗马，不放回未知抽签）。

##### c. 贝叶斯公式

$$
P(A|B) = \frac{P(A)P(B|A)}{P(B)}
$$

贝叶斯公式是打开真实世界的一把钥匙。首先我们思考一个问题：**<u>在真实世界中真的存在非条件概率吗 ？</u>**我个人觉得不存在，也就是说世界上所有概率本质都是条件概率，但是由于条件的错综复杂性，很难直接获得准确、真实的条件概率值，那我们就需要一步步地细化认知，怎么做呢 ？按照如下的方式（最经典的例子莫过于[狼来了](https://www.jianshu.com/p/9957ff9b2f6f#)的故事，以及检查疾病的例子）。

```c++
Step 1: 依靠过去的经验，对事件 A 的发生有着基本的判断 => 通过过往认知，能够得到事件 A 发生的先验概率为 P(A)
Step 2: 但是这个先验概率可能并不是十分准确的，因为真实世界中可能根本不存在非条件概率。
				A 的发生总会和其他一些事情相牵连，所以我们总会充分获取一些事情（比如 B）的相关信息来对 A 发生的概率进行调整
Step 3: 计算后验概率 P(A|B)，之所以叫做后验概率就是因为实在获取 B 事件之后才调整出的概率。

Step 1 to 3 在真实世界中是不断循环往复，以获取尽可能准确的后验概率。

// ------------------------------ // 
教材上讲了狼来了的故事，我们在这再补充一个检查肝癌的例子。
  - 已经知道一个村子的肝癌发生概率（先验概率 P(L)），这个概率是通过过往的经验来的（比如过去 100 年一个共有多少人得了）
  - 但是只有这个先验概率，我们只能得出一个结论：每个人都有 P(L) 的几率罹患肝癌，这个概率是没有意义的。
  	因此，我们要找和肝癌相关联的事件，科学家们历经千辛万苦找到了和肝癌具有一定“因果关系”（或许只是相关关系）的一种试剂检测方式
  - 这样我们就可以计算后验概率 P(L|A) 以及 P(L|~A) 帮助甄别肝癌患病概率。当然，这要求我们有着精确的 P(A); P(A|L); P(A|~L);
		· P(A) 就是我们之前已经获取的不那么精准的先验概率;
		· P(A|L) 和 P(A|~L) 都可以通过实验得出。
```

#### 1.3.3 独立性

##### a. 定义

通过条件概率公式我们可以推得如果一个事件 $A$ 的先验概率 $P(A)$ 等于在某事件 $B$ 下的后验概率 $P(A|B)$，那么我们就说 $A、B$ 二者独立。更进一步地，如果满足下式子，即说明**<u>两个事件相互独立</u>**。注意，这个公式是判断事件是否独立的唯一方法，千万不要依靠直觉去判断！
$$
P(AB) = P(A)P(B)
$$
==拓展来看（多个事件的相互独立性）==：设有 $n$ 个事件 $A_1,A_2,\cdots,A_n$，对**<u>任意的</u>** $1\le i<j<k\cdots\le n$，如果以下等式**<u>均成立</u>**
$$
\left\{
      \begin{aligned}
        & P(A_iA_j) = P(A_i) P(A_j), \\
        & P(A_iA_jA_k) = P(A_i) P(A_j) P(A_k), \\
        & \vdots \\
        & P(A_1A_2\cdots A_n) = P(A_1)P(A_2) \cdots P(A_n),
      \end{aligned}
    \right.
$$
则称此 $n$ 个事件$A_1,A_2,\cdots,A_n$ **<u>相互独立</u>**。拿 3 个事件举例子：设 $A,B,C$ 是三个随机事件，如果有
$$
   \left\{
      \begin{aligned}
        & P(AB) = P(A) P(B), \\
        & P(AC) = P(A) P(C), \\
        & P(BC) = P(B) P(C),
      \end{aligned}
    \right.
$$
则只能得到 $A,B,C$ **<u>两两独立</u>**，若还有
$$
P(ABC) = P(A) P(B) P(C)
$$
才能得到 $A,B,C$ **<u>相互独立</u>**。

##### b. 性质

- **<u>连锁反应</u>**：如果 A 和 B 独立，那么 A 反、B 反、A、B 的各种组合都独立不要怀疑！
- **<u>必定独立</u>**：概率为 0 的时间以及概率为 1 的事件与任意一个事件均相互独立；
- A 和 B 相互独立，有：$P(A|B) = P(A|\overline B) = P(A)$，反之也成立；  
- 互斥不一定独立，独立不一定互斥。二者本质上没有任何联系，根据公式就可以看出，只要 A 和 B 的概率同时都大于零那独立和互斥不可能同时出现。

### 1.4 考点与典例

#### 1.4.1 考点 （量化笔试就曾出现过考研原题）

- 随机事件的运算（本质是集合的计算以及集合关系的判断，利用各种集合运算定律<尤其是德摩根>，最简单的技巧就是画出集合 的VN 图）
- 古典概型排列组合，几何概型画图计算
- 有关概率的各种证明（三个概率的加法公式），尤其是条件概率的证明，是对乘法公式、贝叶斯公式的深入考察
- 全概率公式和贝叶斯公式
- 根据独立计算概率 => 伯努利古典概型

#### 1.4.2 典例

- 三个事件的加法公式 （千万不要嫌麻烦，硬算就是正确的道路）

  <img src="/Users/karry/Pictures/NoteImages/image-20230814000552019.png" alt="image-20230814000552019" style="zoom:40%;" />

- 小题要学会举特例

  > 假设必然事件或不可能事件，下面这道题目就可以先举 B 是不可能事件，再举 B 是必然事件
  
  <img src="/Users/karry/Pictures/NoteImages/image-20230814001745612.png" alt="image-20230814001745612" style="zoom:40%;" />

------



## 第二章 随机变量及其分布

> 第一章我们充分认识了随机事件的含义，但为了更好的进行数学处理，只用随机事件这一个工具来处理随机现象是不够的，因为事件始终是一种定性的表示。为了进行定量的数学处理，必须把随机现象的结果数量化，这就是引入随机变量的原因。

### 2.1 随机变量及其分布的概念

#### 2.1.1 随机变量

==【定义】随机变量==：定义在样本空间 $\Omega$ 的实值函数 $X = X(\omega)$ 称为**随机变量**，常用大写字母 $X, Y, Z$ 等表示随机变量，其取值用小写字母 $x,y,z$ 等表示。这表明：随机变量 $X$ 是样本点 $\omega$ 的**一个函数**

- 函数既可以是不同样本对应不同的实数
- 函数也可以是多个样本点对应同一个实数

函数的自变量（样本点）可以是数，也可以不是数，**但因变量一定是实数。**==随机变量定义的本质，是将随机事件的样本空间映射到实数空间上（随机事件的数量化）==。根据所映射的实数空间的特点，可以分为**离散随机变量**和**连续随机变量**。

注意，与微积分的变量不同，概率论中的随机变量 $X$ 是一种 “随机取值的变量且伴随着一个分布”，也就是说**我们不仅要知道 $X$ 可能取哪些值，而且还要知道取这些值的概率各是多少，更重要的我们知道取这些值的概率和肯定为 1。** 因此有没有分布是区分一般变量与随机变量的主要标志。

#### 2.1.2 分布函数

##### a. 定义

分布函数是将随机事件的概率与随机变量相连接的最直接的工具。设 $X$ 是一个随机变量，对任意实数 $x$，称
$$
F(x)=P(X \leqslant x)
$$
为随机变量 $X$ 的**分布函数**，且称 $X$ 服从 $F(x)$，记为 $X\sim F(x)$，有时也可用 $F_{X}(x)$ 以表明是 $X$ 的分布函数（把 $X$ 作为 $F$ 的下标）。==分布函数的概念核心是“累积”。==

##### b. 性质

任一分布函数 $F(x)$ 都具有如下三条基本性质：

- **单调性**： $ F(x) $ 是定义在整个实数轴 $ (-\infty,+\infty ) $ 上的**单调非减函数**，即对任意的 $x_1 < x_2$，有 $F(x_1)\leq F(x_2)$。

- **有界性（规范性）**：对任意的 $ x $，有 $0\leq F(x)\leq 1 $，且
  $$
  \begin{array}{l}{F(-\infty)=\lim _{x \rightarrow-\infty} F(x)=0}, \\ {F(+\infty)=\lim _{x \rightarrow+\infty} F(x)=1}.\end{array}
  $$

- **右连续性**：$ F(x) $ 是 $ x $ 的右连续函数，即对任意的 $ x_0 $，有
  $$
  \lim _{x \rightarrow x_{0}+} F(x)=F\left(x_{0}\right)
  $$
  即
  $$
  F\left(x_{0}+0\right)=F\left(x_{0}\right)
  $$
  这也就意味着==分布函数的定义域一定是**左闭右开**==，左边的边界一定能包含进来，右边的边界一定无法包含进来（如果包含进来就说明右边界点的+极限小也可以包含进来，就矛盾了）。尽管对于连续函数而言计算上都是相等的，但是概念上千万不可以错。

以上三条性质都可以从定义直接推出（因为定义中包含概率，所以可以借助概率的三条公理）。需要注意的是，==这三条基本性质是判断某个函数是否能成为分布函数的充要条件。==下面我们就具体来看一下，随机变量的分布函数的形式。

#### 2.1.3 分布函数的具体形式

##### a. 离散变量

对于离散变量而言，其所有可能的取值都是可列的，因此通过直接进行穷举就可以得到之前固有的概率表示形式，称之为**分布列**。其能够直观地表现出概率的大小关系，直接显示出哪些地方概率大，哪些地方概率小。
$$
\begin{array}{c|ccccc}
	X	&    x_{1}     &    x_{2}     &    \cdots     &     x_{n}    &   \cdots \\ \hline
	P	&    p(x_1)     &    p(x_2)     &    \cdots     &    p(x_n)     &   \cdots \\
\end{array}
$$
根据定义，我们也可以直接得到离散随机变量的分布函数

<img src="/Users/karry/Pictures/NoteImages/image-20230817145045971.png" alt="image-20230817145045971" style="zoom:40%;" />

从这个地方我们就可以看出来，离散随机变量的分布列和分布函数都可以直接从概率推出来。

##### b. 连续变量

对连续变量而言，其所有可能的取值不可列，只能在数轴上进行连续地呈现，不存在分布列的概念，因此给出如下定义：设随机变量 $X$ 的分布函数为 $F_X(x)$，如果存在实数轴上的一个非负可积函数 $ p(x) $ ，使得对任意实数 $ x $ 有
$$
F(x)=\int_{-\infty}^{x} p(t) d t,
$$
则称 $p(x)$ 为 $X$ 的**概率密度函数**。概率密度函数的两大性质：

- 始终 $\ge 0$
- 积分为 $1$

通过引入概率密度函数的概念，**采用图形的面积直观展现概率大小**，能够达到类似于分布列的效果，但在其本质含义上天差地别。尤其是下面的一个点：

离散随机变量 $ X $ 在其可能取值的点 $x_{1}, x_{2}, \cdots, x_{n}, \cdots$ 上的概率不为 $ 0 $ ，而连续随机变量 $ X $ 在 $(-\infty,+\infty)$ 上**任一点 $ a $ **的概率恒为 $ 0 $ 
$$
P(X=a)=\int_{a}^{a} p(x) d  x=0
$$
这表明：

- 不可能事件的概率为 $ 0 $，但==概率为 $0$ 的事件不一定是不可能事件==（一个点）
- 必然事件的概率为 $ 1 $ ，但==概率为 $ 1 $ 的事件不一定是必然事件==（连续样本空间挖掉一个点）

这也进一步呈现出了概率论和微积分的不同之处，在有限个点上（概率为 $0$ 的部分）做操作对函数的性质没有任何的影响，也就是说在概率论中可以提出概率为 $0$ 的事件后讨论两个函数相等以及其他随机问题。

### 2.2 随机变量的数字特征

#### 2.2.1 $k$ 阶矩

设 $X$ 为随机变量，$k$ 为正整数。如果以下的数学期望都存在，则称

- $\mu_k = E(X^k)$ 为 $X$ 的 ==$k$ 阶**原点矩**==；
- $\nu_k = E[X - E(X)]^k$ 为 $X$ 的 ==$k$ 阶**中心矩**==。

可以看出，这是对随机变量基本数据特征的汇总，后续所有的数字特征都是通过这个概念衍生出来的，所谓矩就是距离。

#### 2.2.2 数学期望（1 阶原点矩）

##### a. 定义

数学期望的本质是**随机变量的值通过与概率的大小（发生的可能性）进行加权平均**，表示分布所处位置的特征数，刻画了 $X$ 的取值始终在 $E(x)$ 的周围波动，进而在一定程度上消除随机变量的随机性。

针对离散和连续随机变量有不同的表述形式，但是其本质含义是相通的。我们在此给出连续随机变量的期望定义，离散随机变量只需将**积分变为级数求和**。设连续随机变量 $X$ 的密度函数为 $p(x)$，如果
$$
\int_{-\infty}^{+\infty}|x| p(x) d  x<+\infty
$$
则称
$$
E(X)=\int_{-\infty}^{+\infty} x p(x) d  x 
$$

为 $ X $ 的**数学期望**，或称为该分布 $ p(x) $ 的数学期望，简称期望或均值。 若$\int_{-\infty}^{+\infty}|x| p(x) d  x$不收敛，则称 $ X $ 的数学期望不存在。

##### b. 性质

数学期望的核心性质为：
$$
E[g(X)] = \begin{cases}
	  \sum_{i} g\left(x_{i}\right) p\left(x_{i}\right), & \text{在离散场合；} \\
      \int_{-\infty}^{+\infty} g(x) p(x) d  x, & \text{在连续场合.}
	\end{cases}
$$
进而推导出：

- $E(c) = c$
- $E ( a X ) = a E ( X )$

#### 2.2.3 方差 & 标准差 （2 阶中心矩）

##### a. 定义

方差的本质是**随机变量的值与期望的差的平方（离散程度）的期望**，表示随机变量的波动程度。若随机变量 $ X^2 $ 的数学期望 $ E(X^2) $ 存在，则称偏差平方 $ (X-E(X))^2 $ 的数学期望 $ E(X-E(X))^2 $ 为随机变量$ X $(或相应分布)的方差，记为：
$$
Var (X)=E(X-E(X))^{2}
   =\begin{cases}
\sum_{i}[x_{i}-E(X)]^{2} p\left(x_{i}\right), & \text{在离散场合；}\\
\int_{-\infty}^{+\infty}[x-E(X)]^{2} p(x) d  x, & \text{在连续场合.}
\end{cases}\label{eq:2.3.1}
$$
称方差的正平方根 $\sqrt{Var (X)}$ 为随机变量 $ X $（或相应分布）的标准差,记为 $\sigma(X)$，或 $\sigma_X$。需要注意的是：==如果随机变量 $X$ 的数学期望存在，其方差不一定存在；而当 $ X $ 的方差存在时，则 $ E(X) $ 必定存在==，其原因在于 $|x| \leqslant x^{2}+1$ 总是成立的。

##### b. 性质

- $Var(X)=E(X^2)-[E(X)]^{2}$ 求解方差的常用方法。
- $Var (c)=E(c-E(c))^{2}=E(c-c)^{2}=0$
- $Var (a X+b)=a^{2} Var (X)$

##### c. 切比雪夫不等式

世间万物均有回归现象，随机变量其本质也是趋近于均值的回归，与均值偏差较大的事件发生的概率可能性要更小。因此随机变量的可能取值、均值以及方差之间存在一个很显然的不等式来表示这种回归约束，这就是切比雪夫不等式的现实含义。

设随机变量 $ X $ 的数学期望和方差都存在，则对任意常数 $\varepsilon>0 $，有：
$$
P(|X-E(X)| \geqslant \epsilon) \leqslant \frac{Var (X)}{\varepsilon^{2}}
$$
更进一步的解释如下：在概率论中，事件“$|X-E(X)| \geqslant \varepsilon$ ” 称为**大偏差**，其概率 $ P(|X-E(X)| \geqslant \varepsilon) $ 称为**大偏差发生概率**，切比雪夫不等式给出**大偏差发生概率的上界**，这个上界与方差成正比，方差愈大上界也愈大，方差愈小随机变量的取值更加集中在均值附近。

#### 2.2.3 变异系数

方差（或标准差）反映了随机变量取值的波动程度，但在比较两个随机变量的波动大小时，如果仅看方差（或标准差）的大小有时会产生不合理的现象。这有两个原因（说白了就是因为**绝对的大小进行比较没有任何实际意义**）

- 随机变量的取值**有量纲**，不同量纲的随机变量用其方差（或标准差）去比较它们的波动大小不太合理
- 在取值的量纲相同的情况下，取值的大小有一个**相对性问题**，取值较大的随机变量的方差（或标准差）也允许大一些

所以要比较两个随机变量的波动大小时，在有些场合使用以下定义的变异系数来进行比较，更具可比性。设随机变量 $X$ 的二阶矩存在，则称比值
$$
C_v(X) = \frac{\sqrt{Var(X)}}{E(X)} = \frac{\sigma(X)}{E(X)}
$$
为 $X$ 的**变异系数**。因为变异系数是以其数学期望为单位去度量随机变量取值波动程度的特征数，标准差的量纲与数学期望的量纲是一致的，所以**变异系数是一个无量纲的量**。

#### 2.2.4 分位数（$p$ 值、中位数）

设连续随机变量 $X$ 的分布函数为 $F(x)$，密度函数为 $p(x)$，对任意 $p\in(0,1)$，称满足条件
$$
F(x_p) = \int_{-\infty}^{x_p}p(x)d x = p
$$
的 $x_p$ 为此分布的 **$p$ 分位数**，又称**下侧 $p$ 分位数**。

#### 2.2.5 偏度 & 峰度

这两个特征数是**描述分布形状的特征数**，本质是**相对特征**，都是标准正态分布为基准。标准正态分布的偏度和峰度都是 $0$。 在实际中，一个分布**标准化后的偏度和峰度**皆为 $0$ 或近似为 $0$ 时，常认为该分布为正态分布或近似为正态分布。

##### a. 偏度

设随机变量 $X$ 的三阶矩存在，则称比值
$$
\beta_s = \frac{E[X-E(X)]^3}{[E(X-E(X))^2]^{3/2}} = \frac{\nu_3}{(\nu_2)^{3/2}}
$$
为 $X$ 的分布的**偏度系数（偏度）**，是描述分布偏离对称程度的一个特征数。

- 当 $\beta_s>0$ 时，分布为正偏或右偏
- 当 $\beta_s=0$ 时，分布关于其均值 $E(X)$ 对称
- 当 $\beta_s<0$ 时，分布为负偏或左偏

<img src="/Users/karry/Pictures/NoteImages/image-20230817180946649.png" alt="image-20230817180946649" style="zoom:46%;" />

##### b. 峰度

设随机变量 $X$ 的四阶矩存在，则称比值
$$
\beta_k = \frac{E[X-E(X)]^4}{[E(X-E(X))^2]^2} - 3 = \frac{\nu_4}{(\nu_2)^2} -3
$$
为 $X$ 的分布的**峰度系数**，是描述分布尖锐程度和尾部粗细的一个特征数，注意不是分布的峰值高低（稍加计算就会发现正态分布的峰度和其峰值完全无关）。

- 当 $\beta_k<0$ 时，则标准化后的分布尖锐程度比标准正态分布更平坦，称为低峰度
- 当 $\beta_2=0$ 时，则标准化后的分布尖锐程度与标准正态分布相当
- 当 $\beta_2>0$ 时，则标准化后的分布尖锐程度比标准正态分布更尖峭，称为高峰度

<img src="/Users/karry/Pictures/NoteImages/image-20230817181517023.png" alt="image-20230817181517023" style="zoom:40%;" />

### 2.3 常用分布

通过对现实世界的观察，可以总结发现很多常见的分布，在此对概念和常用性质做一个总结，需要经常进行记忆。后面则细化补充说明了一些常见分布的特点，并辅之以相关题目。

<img src="/Users/karry/Pictures/NoteImages/image-20230817162455734.png" alt="image-20230817162455734 " style="zoom:33%;" />

<img src="/Users/karry/Pictures/NoteImages/image-20230817162526942.png" alt="image-20230817162526942 " style="zoom:33%;" />

#### 2.3.1 离散分布

##### a. 泊松分布（Poisson D）

==泊松分布能对现实中的许多现象进行准确描述==：常与单位时间（或单位面积、单位产品等）上的计数过程相联系

- 在一天内来到某商场的顾客数（可用于排队论的假设）
- 一平方米内，玻璃上的气泡数
- 一定时期内，放射性物质放出的粒子数

==泊松定理（二项分布的近似）==：在 $n$ 重伯努利试验中，记事件 $A$ 在一次试验中发生的概率为 $p_n$（与试验次数 $n$ 有关），如果当$n\to+\infty$时，有 $np_n\to\lambda$，则
$$
  \lim_{n\to+\infty} C_n^k p_n^k(1-p_n)^{n-k}
  = \frac{\lambda^k}{k!}e^{-\lambda}.
$$
这样在试验次数 $n$ 很大，而发生概率 $p$ 很小的情况下，就可以将二项分布近似为泊松分布实现简化计算。

##### b. 负二项分布（帕斯卡分布）

==定义==：在伯努利试验序列中，记每次试验中事件 $A$ 发生的概率为 $p$，如果==$X$ 为事件 $A$ 第 $r$ 次出现时的试验次数==，则 $X$ 的可能取值为$r,r+1,\cdots,r+m,\cdots$。称 $X$ 服从**负二项分布**，其分布列为
$$
P(X = k) = C_{k-1}^{r-1} p^r(1-p)^{k-r},\; k=r,r+1,\cdots.
$$
记为 $X\sim Nb(r,p)$，当 $r=1$ 时，即为几何分布（几何分布具有无记忆性）。

#### 2.3.2 连续分布

##### a. 正态分布 （最为重要）

$X\sim N(\mu, \sigma)$概率密度函数（配方、除系数、加因子）如下。积分得到的分布函数没有意义，因为积不出来。
$$
f(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
$$
大多数时间都是只分析标准正态分布，标准正态分布的均值和方差是可以积分出来的，进而得到普通正态分布的均值方差。

##### b. 伽马分布

==伽马函数==：
$$
\Gamma(\alpha) = \int_0^{+\infty}x^{\alpha-1}e^{-x} d x
$$
两大性质，辅助求解积分

- $\Gamma(1)=1,\Gamma\left(\frac12\right)=\sqrt\pi$；
- $\Gamma(\alpha+1)=\alpha\Gamma(\alpha)$（可用分部积分法证得）. 当$\alpha$为自然数$n$时，有 $ \Gamma(n+1) = n\Gamma(n) = n!$

##### C. 其他分布及性质

指数分布无记忆性 $P(X > a + b | a) = P(x > b)$，例如学了 10 小时的情况下再学 5 个小时的概率 = 刚开始就学 5 个小时的概率。

### 2.4 随机变量变换的分布

#### 2.4.1 随机变量函数的分布

设 $y=g(x)$ 是定义在直线上的一个函数，$X$ 是一个随机变量，那么 $Y=g(X)$ 作为 $X$ 的函数，同样也是一个随机变量。我们要研究的问题是：已知随机变量 $X$ 的分布，如何求出另一个随机变量 $Y=g(X)$ 的分布。对于离散和连续变量，其本质都是一样的，无非就是按照分布函数的定义进行推导，只不过离散变量比较简单直接求解，连续变量在某些情况下存在特殊法则。

一般情况下直接采用下面的方式对随机变量函数的分布进行求解：

- **<u>*Step 1*</u>** 严格按照定义进行推导（反解 $X$ 的取值，精准解决分段是关键：请务必画个图看看 $X$ 和 $Y$ 之间的关系，找到关键点确定区间！）
  $$
  \begin{align*}
  F_Y(y) & = P\{Y \le y\} \\
  & =P\{g(X) \le y\} \\
  & =P\{\phi(y) \le X \le \gamma(y)\}
  \end{align*}
  $$

- **<u>*Step 2*</u>** 积分求出 $F_Y(y)$
  $$
  F_Y(y) = \int_{\phi(y)}^{\gamma(y)} f_X(x)dx = F_X(\gamma(y)) - F_X(\phi(y))
  $$

- **<u>*Step 3*</u>**  $F_Y(y)$ 对 $y$ 求导得到 $f_Y(y)$
  $$
  f_Y(y) = \frac{dF_Y(y)}{dy}
  $$

==来一个例子：正态分布标准化的过程。==已知：$X\sim N(\mu, \sigma)$ 证明 $Y = \frac{X-\mu}{\sigma} \sim N(0, 1)$ 

- **<u>*Step 1*</u>** 严格按照定义进行推导
  $$
  \begin{align*}
  F_Y(y) & = P\{Y \le y\} \\
  & =P\{\frac{X-\mu}{\sigma} \le y\} \\
  & =P\{X \le \sigma y + \mu\}
  \end{align*}
  $$

- **<u>*Step 2*</u>** 积分求出 $F_Y(y)$
  $$
  F_Y(y) = \int_{-\infty}^{\sigma y + \mu} f_X(x)dx
  $$

- **<u>*Step 3*</u>** $F_Y(y)$ 对 $y$ 求导得到 $f_Y(y)$
  $$
  f_Y(y) = \frac{dF_Y(y)}{dy} = \sigma f_X(\sigma y + \mu) =\frac{1}{\sqrt{2\pi}}e^{-\frac{y^2}{2}}
  $$

#### 2.4.2 随机变量组合的分布

上面我们求解的是一个随机变量函数变换后的分布情况，但还有另一种情况，那就是随机变量组合的情况。例如 $N$ 个 独立的二项分布随机变量 $X_1, X_2, \cdots, X_N$ 的加和 $\sum_{i=1}^NX_i$ 就服从二项分布。

这也就自然地引出了后续要探讨的多维随机变量分布的学习，以及对世界上各种随机事件组合出新事件的深入思考。

### 2.5 考点与典例

#### 2.5.1 考点 1 分布函数的概念与性质

==1== 判断某些函数是否为分布函数，就是考察定义：1）单调性；2）两端 0 和 1； 3） 右连续。

<img src="/Users/karry/Pictures/NoteImages/image-20230818161537803.png" alt="image-20230818161537803" style="zoom:50%;" />

```c++
【c】 右连续直接秒
```

==2== 根据分布函数求解概率，或者根据概率求解分布函数（套定义，得到公式）

<img src="/Users/karry/Pictures/NoteImages/image-20230818163921081.png" alt="image-20230818163921081" style="zoom:50%;" />

```c++
三步走即可：1. 找到关键点（-1， 1） 2. 划分区间（牢记左闭右开<否则区间没有意义>）3.累积求和
```

#### 2.5.2 考点 2 随机变量的性质

==1== 对离散性随机变量进行判断（下面这道题必须要记住）

<img src="/Users/karry/Pictures/NoteImages/image-20230818165406170.png" alt="image-20230818165406170" style="zoom:50%;" />

> 这道题的求解太 amzing 了，感觉随时都有可能考到，这也说明了高数的重要性 
>
> - **<u>*Step 1*</u>** 给出泊松分布 $P(X = k) = \frac{\lambda^k}{k!}e^{-\lambda}$
>
> - **<u>*Step 2*</u>** 给出取偶数的概率 $P(X = k (k 为 偶数)) = \sum_{i=0}^n(\frac{\lambda^{2i}}{2i!}e^{-\lambda}) = e^{-\lambda}\sum_{i=0}^n\frac{\lambda^{2i}}{2i!}$
>
> - **<u>*Step 3*</u>** 想方设法求解后面的级数，想到泰勒展开
>
>   - $e^x = \sum_{i=0}^n\frac{x^i}{i!}$
>   - $e^{-x} = \sum_{i=0}^n\frac{(-x)^i}{i!}$
>   - $e^x + e^{-x} = \sum_{i=0}^n\frac{2x^{2i}}{2i!}$ 所有偶数项的二倍
>   - $e^x - e^{-x} = \sum_{i=0}^n\frac{2x^{(2i+1)}}{(2i+1)!}$ 所有奇数项的二倍
>
>   因此 $ e^{-\lambda}\sum_{i=0}^n\frac{\lambda^{2i}}{2i!} = e^{-\lambda}(\frac{e^x + e^{-x}}{2}) > \frac{1}{2}$

```c++
【c】 为偶数的概率大于 0.5，所以为偶数的概率大于为奇数的概率
```

==2== 连续随机变量的概率密度函数，和判断分布函数一样的意思。直接套定义，1）始终大于等于 0 ；2） 积分为 1

<img src="/Users/karry/Pictures/NoteImages/image-20230818171910454.png" alt="image-20230818171910454" style="zoom:50%;" />

==3== 指数分布的考点，最直接的考点就是：指数分布的定义

<img src="/Users/karry/Pictures/NoteImages/image-20230818173236416.png" alt="image-20230818173236416" style="zoom:50%;" />
$$
F_T(t) = P\{T \le t\} = 1 - P\{N(t) = 0\} \ \ \ （在\  t\  时间内不发生故障的概率）
$$

```c++
【定义题 + 智力题】只要弄清楚概率分布函数的定义并灵活转换即可 + 指数分布的无记忆性
```

==4== 正态分布辅助计算积分，实际上带 $e^{x^2}$ 的积分直接求都是不可求的，遇到之后都要借助标准正态分布，永远记住：不标准的正态分布根本没办法求解，所有有关正态分布的计算都要标准化。

<img src="/Users/karry/Pictures/NoteImages/image-20230818193432187.png" alt="image-20230818193432187" style="zoom:50%;" />

#### 2.5.3 考点 3 随机变量函数的分布

```c++
这一类题目数不胜数，二维变量也是常见的，我们在前面的知识点处已经梳理了整体的思路，做题用下述步骤
1. 画图，找到 X 和 g(X) 的图像关系
2. 找关键点，划分区间 （划分区间是最重要的，左闭右开）
3. 套用定义完成计算
```

<img src="/Users/karry/Pictures/NoteImages/image-20230818203630616.png" alt="image-20230818203630616" style="zoom:50%;" />

> 划分准区间为：$(-\infty, -1), [-1, 0), [0, +\infty)$

<img src="/Users/karry/Pictures/NoteImages/image-20230818203856632.png" alt="image-20230818203856632" style="zoom:50%;" />

----





## 第三章 多维随机变量

### 3.1 二维随机变量的含义

大多数现实场景中我们都不能只考虑其中一个随机变量，二是多维随机变量共同考虑，最简单的就是二维的情况，对两个变量进行同时考虑。

### 多维随机变量及其分布的定义

`1. 基本性质` 

- 分布函数必须要是非降的，随着 x 和 y 的增加必须值不能变小。

- 和一维完全相同，密度函数大于零，平面积分为 1，分布函数 x, y 趋近于正无穷为 1，趋近于负无穷为零。

- 不论对于 x 还是 y 都是右连续的

- $$
  P(x_1 < X < x_2, y_1 < Y < y_2) = \\
  F(x_2, y_2) - F(x_2, y_1) - F(x_1, y_2) + F(x_1, y_1) \ge 0
  $$

离散变量整体处理起来比较简单，就不再赘述，这一章关键在连续，在多元积分上，积分千万要会求。

<img src="C:\Users\16332\AppData\Roaming\Typora\typora-user-images\image-20230701114032033.png" alt="image-20230701114032033" style="zoom:33%;" />

`2. 连续随机变量的三大函数`

- 概率密度函数，按照定义就是分布的二元微分

- 边缘密度函数，直接从概率密度函数来：X 的边缘密度就对 Y 求积分，Y 的边缘密度就对 X 求积分；或者从分布函数来：求 X 的边缘分布就是让 Y 趋近于无穷大，求 Y 的边缘分布就是让 X 趋近于无穷大。

- 条件密度函数，就是概率密度函数除以边缘密度函数，Conditional 的那个变量是仍然存在的，此时变成了一个超参数。

  <img src="C:\Users\16332\AppData\Roaming\Typora\typora-user-images\image-20230701123228255.png" alt="image-20230701123228255" style="zoom:50%;" />

`3. 两个特殊的二维分布`

- 二维均匀分布：图形面积，十分简单

- 二维正态分布：五个参数 $u_1, u_2, \sigma^2_1, \sigma^2_2,\rho$（最后一个是 X 和 Y 的相关系数）
  $$
  f(x,y) = \frac{1}{\sqrt{2\pi}\sigma_1\sigma_2\sqrt{1-\rho^2}}e^{-\frac{1}{2(1-\rho^2)}[\frac{(x-\mu_1)^2}{\sigma_1^2} + \frac{(x-\mu_2)^2}{\sigma_2^2} - \frac{2\rho(x-\mu_1)(x-\mu_2)}{\sigma_1\sigma_2}]}
  $$
  参考一维正态分布记忆

### 随机变量的独立性

`1. 定义` 注意，定义中对独立性的定义是使用的分布函数，也就是说如果有：
$$
F(x, y) = F(x)F(y)
$$
那么随机变量  X 和 随机变量 Y 就是相互独立的，但是推导可以发现，对于连续函数而言，上述条件等价于：
$$
f(x,y) = f_X(x)f_Y(y)
$$
因此催生了一大批判断是否独立的题目，基本上来说和上面三大函数的求解是放在一起的。但是选择题中如果遇到，一定要用技巧：

- 先看定义域 X 和 Y 一定要不相关
- 再看 $f(x,y)$ 是不是 $g(x)h(y)$ 的形式

`2. 性质` 如果随机变量 X 和 Y 相互独立，且 $g(x), h(y)$ 是连续函数，那么由此得到的新随机变量 $g(X),h(Y)$ 相互独立。

### 二维随机变量的分布转换

`1. 定义`：给定 $Z = G(X)H(Y)$ 问 $Z$ 的分布怎么样。

`2. 方法`：

- 直接用积分方法，一定都要从分布函数出发！套定义先求 $F_Z(z)$ 再求 $f(z)$ 一定要学会分范围求解
- 公式法，有两个常见的 $Z$：
  - $Z = min\{X, Y\} => F_Z(z) = 1-[1-F_X(z)][1-F_Y(z)]$ 
  - $Z = max\{X, Y\} => F_Z(z) = F_X(z)F_Y(z)$
- 特殊性质，正态分布的可加性均值和方差直接相加即可（常数均值为其值，方差为 0）

### 常见考点与经典例题

`1. 常见考点`

- 联合分布函数和概率密度函数的定义与性质
- 求边缘分布函数（从边缘分布出发以及从概率密度出发）连续和离散的都要会
- 随机变量函数的分布

`2. 一个脑筋急转弯的证明，但也切实体现了基础` 设 $Y_1$\~$P(\lambda_1)$，$Y_2$\~$P(\lambda_2)$，且 $Y_1, Y_2$ 相互独立，证明 $Y_1 + Y_2$ \~ $P(\lambda_1+\lambda_2)$：

证明：因为 $P(Y_1 = m) = \frac{\lambda_1^m}{m!}e^{-\lambda_1}$，$P(Y_2 = n) = \frac{\lambda_2^n}{n!}e^{-\lambda_2}$，所以：
$$
P(Y_1 + Y_2 = k) = \sum_{i=0}^k P(Y_1=i)P(Y_2=k-i)
$$
然后进行继续推导，如果反过来看得时候还是不会，看符玺书 P53



## 数字特征

> 均值、方差、协方差 => 相关系数、矩

### 数学期望（均值）

`1. 定义` 每一个可能值 * 其对应的概率：

- 对于离散就是相乘加和
- 对于连续就是积分，函数变化后，就直接对目标函数进行积分即可，比如 $E(X^2),E(sin(X))$，对于依托于二维随机变量的变量可以通过二维积分进行求解。

`2. 性质`：

- 设 C 是一个常数，$E(C) = C$
- 设 X 是一个随机变量，C 是一个常数，$E(CX) = CE(X)$
- 线性可加，设 X，Y 是两个随机变量，则有：$E(aX\pm Y) = aE(X)\pm E(Y)$，再加亦可
- 若随机变量 X，Y 相互独立，则 $E(XY) = E(X)E(Y)$，再加亦可**反之则不然！！！！**只能说明线性不相关罢了

### 方差

`1. 定义` 偏离均值的程度：
$$
D(X) = E\{[X - E(X)]^2\} \\
D(X) = E(X^2) -E(X)^2
$$
因此说白了，还是和求均值一样，只不过 $E(X^2)$ 多了个平方而已

`2. 性质`：

- 设 C 是常数，则 $D(C) = 0$，因此如果 $D(X) = 0$，那么就一定有 $P\{X = E(X)\} = 1$  也就是说，X 始终都是一个值

- 设 X 是一个随机变量，a、b 是常数，那么就有 $D(aX + b) = a^2D(X)$

- 设 X，Y 是两个随机变量，则有：
  $$
  D(X\pm Y) = D(X) + D(Y) \pm 2Cov(X, Y)
  $$
  其中 $Cov(X, Y) = E(XY) - E(X)E(Y)$

- 若 $X, Y$ 相互独立，则 $D(X\pm Y) =D(X) \pm D(Y)$ 反之则不然，还是只能说明线性不相关。

### 协方差

`1. 定义` 衡量多个变量之间的线性相关关系：
$$
Cov(X, Y) = E(XY) - E(X)E(Y)
$$
这个的计算仍然是类似地，无非是在多元概率密度函数中求值罢了，注意如果是二元变量的话，求解 $E(X)$ 或 $E(Y)$ 有两种方法，但是为了简单起见，就直接在原函数上进行积分即可。

`2. 性质`

- $Cov(X, X) = D(X)$

- $Cov(X, Y) =Cov(Y, X)$

- $C$ 是一个常数，那么 $Cov(X, C) = 0$

- $a,b$ 为常数时，$Cov(aX,bY) = abConv$

- 完全线性可加：
  $$
  Cov(a_1X_1 + a_2X_2, b_1Y_1+a_2Y_2) = \\ a_1b_1Cov(X_1, Y_1) + a_1b_2Cov(X_1,Y_2) + a_2b_1Cov(X_2,Y_1) + a_2b_2Cov(X_2,Y_2)
  $$

- 三项情况：很重要！
  $$
  D(X + Y + Z) = D(X) + D(Y) + D(Z) +2Cov(X, Y) + 2Cov(X, Z) + 2Cov(Y, Z)
  $$

### （线性）相关系数

`1. 定义` 当时学习时可能理解得还没有那么深刻，后面来看，所谓相关系数在此处仅仅代表线性相关系数，只衡量两个变量之间的线性相关关系，定义公式如下：
$$
\rho_{XY} = \frac{Cov(X,Y)}{\sqrt{D(X)}\sqrt{D(Y)}}
$$
`2. 性质`

- 相关系数的绝对值小于等于 1 ，也就是说其值在 -1 和 1 之间，如果值等于 0 说明无线性相关性。还是要注意，这只是线性不相关，而非独立！！！

- 一个很重要的性质，经常考察：相关系数为 1 的意思是，二者的关系完全线性
  $$
  P\{Y = aX + b\} = 1 (a\ne0)
  $$

- 再次强调：一般地，X 和 Y 相互独立一定有 X 和 Y 不相关，但不相关只是线性不相关而不是独立

`3. 常考点`

- 很容易验证不相关，但一般来说验证独立很困难（参照上一章所学方法，需要求出两个变量相乘取得的新变量，这个过程极为困难）
- 因此如果出到这类的题目，一般来说答案都只会是，不相关而非独立！验证独立只需要求个二次相关或者三次相关！

### 矩

`1. 定义` 矩就是对所有数字特征的特征化表示，上述三大数字特征均可用矩来表征。

`2. 例子`

- k 阶原点矩：$E(X^k)$，当 $k=1$ 时表示均值。
- k 阶中心距：$E([X-E(X)]^k)$， 当 $k=2$ 时表示方差。
- k + l 阶混合矩：$E(X^kY^l) = E([X-E(X)]^k[Y-E(Y)]^l)$，当 $k=l=1$ 时表示协方差。

### Summary

### 常见考点与相关习题

- 求随机变量函数的数学期望（没办法，只能套公式）
- 求符合变量函数的数学期望（min(X, Y)，max(X, Y)等）（就是设 U=G(x,y) 求解得到 U 的概率密度函数，然后得到期望值）
- 计算方差协方差相关系数并基于此判断是否相关 or 独立（一般独立很难验证出来，都是不相关）
- 有关正态分布的相关概念：不独立的正态分布组合的二元分布可能不是多元正态分布

<img src="C:\Users\16332\AppData\Roaming\Typora\typora-user-images\image-20230701142617173.png" alt="image-20230701142617173" style="zoom:50%;" />



## 大数定律和中心极限定理

### 大数定律

`定义`：当实验次数足够多的时候，整体实验的均值就会趋近于一个定值

#### 切比雪夫不等式

设随机变量的数学期望与方差都存在，则对任意的无穷小都有：
$$
P\{|X - \mu| \ge \epsilon\} \le \frac{\sigma^2}{\epsilon^2} \\
P\{|X- \mu| \le \epsilon \} \ge 1- \frac{\sigma^2}{\epsilon^2}
$$

切比雪夫不等式可以实现：大致估计某一范围内的概率。

#### 依概率收敛

$$
lim_{n\to\infin}\{|Y_n - a| < \epsilon\} = 1
$$

就说序列 $Y_i$ 依概率收敛至 $a$，对于单个变量没有任何意义，但是在大数定律中却可以得到充分有效的使用。注意和趋近于 a 的区别。

#### 三个大数定律

大数定律是描述大量现象平均结果稳定性的定理。

`1. 最 general 的` 切比雪夫大数定律：如果 $X_1, X_2, \cdots, X_n,\cdots$ 是相互独立的随机变量序列，并且每个 $X_i$ 均有有限的数学期望 $E(X_i)$ 和有限的方差 $D(X_i)$，则有：
$$
lim_{n\to\infin}P\{|\frac{1}{n}\sum_{i=1}^n(X_i) - \frac{1}{n}\sum_{i=1}^nE(X_i)|\le\epsilon\} = 1
$$
也就意味着，只要能够给出无穷多个分布的均值，就可以求解前面序列的依概率收敛结果，但是这个在现实中不太好用。

`2. 加一个同分布的` 辛钦大数定律：如果上述相互独立的随机变量同分布，且均值 $E(X_i) = \mu$ 那么就有：
$$
lim_{n\to\infin}P\{|\frac{1}{n}\sum_{i=1}^n(X_i) - \mu|\le\epsilon\} = 1
$$
借助此，只要给出该分布的均值，即可完成对序列依概率收敛值的求解。由此还有一个推论，如果 $E(X_i^k) = \mu_k$ 那么就有
$$
lim_{n\to\infin}P\{|\frac{1}{n}\sum_{i=1}^n(X_i^k) - \mu_k|\le\epsilon\} = 1
$$
这个是常考的点，比如问 $\frac{1}{n}\sum_{i=1}^n(X_i^2)$ 依概率收敛于多少，就要求$E(X^2) = D(X) + E(X)^2$ 

`3. 一个特殊情形` 伯努利大数定律：简单的随机抽样情况，设 $n_A$ 为在 $n$ 次独立实验中事件 $A$ 发生的频数，$p$ 是事件 A 在每次实验中发生的概率，则有：
$$
lim_{n\to\infin} P\{|\frac{n_A}{n} - p| < \epsilon\} = 1
$$
也就是说：随着实验次数的增加，事件发生的频率越来越接近于事件发生的概率。

### 中心极限定理（实际应用必考点）

`定义` 数字逐渐变大，特征逐渐逼近分布中心 —— 正态分布

#### 独立同分布中心极限定理（列维-林德伯格定理）

设随机变量 $X_1,X_2,\cdots,X_n,\cdots$ 独立同分布，且均值为 $\mu$ 方差为 $\sigma^2$，则有：
$$
\sum_{i=1}^n X_i 近似服从 N(n\mu,n\sigma^2)
$$

#### 二项分布以正态分布为极限分布的中心极限定理（棣莫弗-拉普拉斯定理）

二项分布本就是多次 $0-1$ 分布聚集，因此这算是对上面定理的一个**特殊化推广**：设随机变量 $X$ 服从 $B(n, p)$，则当 $n$ 充分大的时候，$X$ 近似服从 $N(np, np(1-p))$

#### 必考题目

给一个实际应用场景，可以是 n 个独立同分布给均值和方差，也可以就是一个二项分布，比如要想让 100 车床正常工作的概率大于 p 至少需要多少电？每张车床工作独立，正常工作概率为 0.6

`Step 1 由基础分布转化为正态分布：`

设同时工作的车床为 X，X 服从 B(100, 0.6)，进而转化为 N(60, 48)

`Step 2 识别题目含义列出准确算式：`
$$
P\{T > X\} > p \\
P\{\frac{X - \mu}{\sigma} < \frac{T-\mu}{\sigma}\} > p \\
P\{\frac{X - 60}{\sqrt{48}} < \frac{T-60}{\sqrt{48}}\} > p \\ 
\Phi(\frac{T-60}{\sqrt{48}}) > p
$$
`Step 3 根据题目给出的分位点，精准求解：` 

一般来说 p 都会以分位点的形式给出，这样只需要求解一个简单的二次或者一次方程即可。



## 数理统计的基本概念（进入统计）

### 总体、样本以及样本统计量

#### 总体的定义和性质

`定义`：之前我们的讨论始终围绕着总体 $X$ 的分布进行，但在实在的世界中我们很难看到总体。所以我们在聊统计时，无非聊两个问题：**<u>理论推导得出</u>**总体服从某一分布，可以得到样本相关数字特征；看到样本服从某一特性推测总体服从某一分布，或者样本是否显著。这就是总体和样本之间的关系。

`性质`

1. 独立性：从总体中抽取的样本之间相互独立
2. 同分布：抽取的样本之间同分布
3. 表示方法：从总体 $X$ 中抽取得到的样本记作 $X_1,X_2\cdots,X_n$ 这些样本的实际观测值记为 $x_1, x_2, \cdots, x_n$

#### 统计量

`定义`：能够用总体已知（绝不可能是未知）参数表征的样本数字特征，都是统计量，样本每次抽取都不一样，统计量也各有不同。

`常见统计量（重点！）`

1. 样本的均值：$\overline{X} = \frac{1}{n}\sum_{i=1}^n(X_i)$
2. 样本的方差：$S^2 = \frac{1}{n-1}\sum_{i=1}^n(X_i-\overline{X})^2$ 这里是 $n-1$ 而不是 $n$ 的原因我们已经深刻了解到了自由度和含义。
3. 当然和总体数字特征一样，也会拥有标准差、原点矩（-0）、中心矩（-均值）但都不重要

`最重要的特征！！！！`	设总体 $X$ 的均值为 $E(X)$ 方差为 $D(X)$

1. 样本均值的期望：$E(\overline{X}) = E(X)$
2. 样本均值的方差：$D(\overline{X}) = \frac{1}{n}D(X)$
3. 样本方差的期望：$E(S^2) = D(X)$ 保证了无偏性
4. 样本方差的方差：$D(S^2)$ 可用卡方分布求解

### 各种由正态分布组合的出来的新分布

#### 卡方分布

`定义`：来自<u>**正态分布**</u>总体的n个随机变量**<u>平方的和</u>**，就是一个服从**<u>自由度为 n</u>**的卡方分布。

`性质`

1. 自由度为 n 的卡方分布的均值为 n，方差为 2n（数字特征，也很好证明，卡方分布本质是一种 Γ 分布）
2. 自由度分别为 n1、n2 的独立卡方分布加和服从自由度为 n1+n2 的卡方分布（用以求解分布中的未知参数，进而使得某分布服从卡方分布）

#### t 分布（单变量检验）

`定义`：由两个独立的分布 X，Y 组合得到，其中 X~N(0, 1)，Y~自由度为 n 的卡方分布，则 $\frac{X}{\sqrt{Y/n}}$ 服从自由度为 n 的 t 分布（谨记！）。

#### F 分布（多变量检验）

`定义`：由两个相互独立的卡方分布，一个为 X\~自由度(n1)的卡方分布，一个为Y\~ 自由度(n2)的卡方分布，组合得到 $F = \frac{X/n1}{Y/n2}$ 服从 F(n1, n2)。

`性质`：和 t 分布互动一下，若 T\~t(n) 则 $T^2$~F(1,n)

#### 分位点（查表）

只要记住四种分布的概率分布图，就明白分位点的含义了：

1. 正态分布、t 分布是关于 y 轴对称的；
2. 卡方和 F 分布所有值都大于 0。

### 正态分布总体下的样本分布（后续很多理论知识的基础）

先看单：假设总体 $X$\~$N(\mu,\sigma^2)$，$(X_1, X_2, \cdots, X_n)$ 为来自总体 $X$ 的简单随机样本则：

1. $\overline{X}$ 服从 $N(\mu, \frac{\sigma^2}{n})$

2. $\frac{(n-1)S^2}{\sigma^2}$ 服从自由度为`n-1​的卡方分布`。（这个定理的详细推导过程并不好做，但是直观能感受出来，借助该结论可以求解很多结论！）

3. 一个 t 分布（这是计量中做 t 检验的基础理论！）
   $$
   T = \frac{\overline{X} - \mu}{S/\sqrt{n}}
   $$
   服从自由度为 $n-1$ 的 t 分布，这个十分重要！

正是因为有了这些基础，就可以在总体和个体之间搭建分布进行详细计算了。

再看双：假设总体 $X$\~$N(\mu_1, \sigma_1^2)$ ，$Y$\~$N(\mu_2, \sigma_2^2)$ 从两总体中进行抽样，得到抽样结果，仍然可以构造相关统计量：

1. $$
   T = \frac{(\overline{X} - \mu_1) - (\overline{Y} - \mu_2)}{S_w\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} 服从 t(n_1 + n_2 -2)
   $$

   其中：
   $$
   S_w =\sqrt{\frac{(n_1-1)S_1^2 + (n_2-1)S_2^2}{n_1 + n_2 -2}}
   $$

2. $$
   F = \frac{S_1^2/\sigma_1^2}{S_2^2/\sigma^2_2} 服从 F(n_1-1, n_2-1)
   $$




## 参数估计 —— 由样本估计总体参数

> 知道总体大概呈什么分布情况，但是其中有些参数仍未知，所以需要根据所观察到得样本数据进行估计，或者类似于计量经济学对总体函数关系参数做估计。
>
> 注意：估计出来的参数，仍然是一个变量，仍然有均值方差

### 点估计

`定义`：最简单的估计方法，在样本上随便设定一个统计量（估计量），然后根据样本的估计值计算该统计量的值，令该值等于总体理论推导值，进而得出总体中未知变量

#### 矩估计 （采用【矩】这一统计量进行估计）

`理论`

1. 在总体上有：一阶原点矩$E(X)$，二阶原点矩$E(X^2)$，二阶中心矩$D(X)$
2. 在样本上有：一阶原点矩$\overline{X}$，二阶原点矩$\frac{1}{n}\sum_{i=1}^nX^2_i$，二阶中心矩$\frac{1}{n}\sum_{i=1}^n(X_i-\overline{X})^2$

由中心极限定理可得，当 n 趋近于无穷大时，样本矩和总体矩相等，进而反解出目标变量。

`应用`

1. 第一步先看有几个未知参数：如果有 1 个，优先使用低阶矩，如果有 2 个，一阶矩和二阶矩都要用上，二阶矩建议使用二阶中心矩；
2. 先在总体上构建矩估计方程，得到参数如何用总体特征表示；
3. 再在样本上估计出数字特征，反代回 2 得到估计值。

#### 极大似然估计（让看到的样本发生的概率最大化）

`理论`：对于带有未知参数的一个总体分布而言，观测到了一个确定的样本，让该样本发生概率最大的那个参数，才是好参数。

`应用` （尤其注意均匀分布 max 和 min 的情况）

1. Step 1：写出似然函数（离散连续两种情况）

   <img src="C:\Users\16332\AppData\Roaming\Typora\typora-user-images\image-20230430145933148.png" alt="image-20230430145933148" style="zoom:33%;" />

2. Step 2：写出对数

   <img src="C:\Users\16332\AppData\Roaming\Typora\typora-user-images\image-20230430145959587.png" alt="image-20230430145959587" style="zoom:33%;" />

3. Step 3：求解参数

   <img src="C:\Users\16332\AppData\Roaming\Typora\typora-user-images\image-20230430150026697.png" alt="image-20230430150026697" style="zoom:33%;" />

4. Step 4：因为上面仅仅是观测值，不同的样本可能有不同的观测值，因此要把观测值$x_i$换为样本值$X_i$

`最大似然估计的不变性` 就是说如果 $\hat{\theta}$ 是参数 $\theta$ 的最大似然估计值，那么$g(\hat{\theta})$ 也是$g(\theta)$ 的最大似然估计值。也就是说：求解出来估计结果，可以进行函数转换。

#### 点估计选择估计出来的结果的标准

`目的`：两种点估计方法，估计出来的最终结果可能有所不同，到底该怎么选呢？提供了许多的标准。

`无偏性`：$E(\hat{\theta}) = \theta$ 则无偏，最明显的就是说，样本的二阶中心矩并不是无偏的，但样本方差是无偏的。其实无非就是对估计出的参数 $Y$ 进一步求期望，如果 $Y = \overline{X}$ 那就很简单了，但是如果 $Y = max(X_1, X_2,\cdots,X_n)$ 这就麻烦了，需要先把 $Y$ 的分布给出来<可以用公式！>然后再求均值 or 方差

`有效性`：**<u>只有无偏估计量才有资格比较有效性</u>**，两个无偏估计量谁的方差越小谁越有效，因此这个地方就和无偏性对应起来了，上面讲的是均值，此处讲的是方差。求方差的那一套拿过来直接用！

`一致性`：有效性暗含着一个趋势，是指随着样本容量越来越大，样本中所含的分布信息越来越多，估计出的参数越来越接近总体参数。

### 区间估计（一个硬币的两面）

`定义`：给定样本所得到的点估计虽然表面上只是一个确定的值，但实际上其再一个分布中（要不怎么会有均值方差呢，求无偏有效性呢？）我们当然可以根据这个分布来判断估计值可能所在的区间。

`方法（在此我们只考虑正态分布总体）` $N(\mu, \sigma^2)$，凡是不为该分布的，都通过中心极限定理，转变为该分布。

思路其实都是完全相同的：先找一个无偏估计量，然后将此无偏估计量和待估计参数结合起来构造一个常见的分布，然后反解即可。

1. 求 $\mu$ 的区间估计（总体的 $\mu$ 未知，但 $\sigma^2$ 已知）构建**<u>正态分布统计量</u>**

   <img src="C:\Users\16332\AppData\Roaming\Typora\typora-user-images\image-20230430154523267.png" alt="image-20230430154523267" style="zoom:19%;" />

   <img src="C:\Users\16332\AppData\Roaming\Typora\typora-user-images\image-20230430154808491.png" alt="image-20230430154808491" style="zoom:20%;" />

2. 求 $\mu$ 的区间估计（总体的 $\mu,\sigma^2$ 均未知）构建**<u>t 统计量</u>**

   <img src="C:\Users\16332\AppData\Roaming\Typora\typora-user-images\image-20230430155401302.png" alt="image-20230430155401302" style="zoom:33%;" />

   这就是我们在做回归估计参数时，惯用的检验显著与否的手段。

3. 求 $\sigma^2$ 的区间估计（总体的 $\mu,\sigma^2$ 均未知）

   <img src="C:\Users\16332\AppData\Roaming\Typora\typora-user-images\image-20230430161226935.png" alt="image-20230430161226935" style="zoom:30%;" />

   <img src="C:\Users\16332\AppData\Roaming\Typora\typora-user-images\image-20230430161246646.png" alt="image-20230430161246646" style="zoom:25%;" />

4. 求 $\sigma^2$ 的区间估计（总体的 $\mu$ 已知，但 $\sigma^2$ 未知）

   将上面卡方分布的分布改为 $\sum_{i=1}^{n}(x_i-\mu)^2$ 这样可以减少自由度的损失，更加精准。

当然，以上结论都是给的双侧，但是单侧的原理和上述完全相同，只要记住如何构造分布，就什么都不怕！



## 假设检验

> 这和上面一章参数估计是一脉相承的，上一章要求估计出某个参数的准确值。现在我们要根据样本情况，验证某个参数是否满足某些条件，在假设成立的前提下，小概率事件是否发生。

`定义`：从小概率事件是否发生的角度来讲这个故事：不论是参数估计还是假设检验，我们针对的始终是总体的参数。在不知道总体的某个参数时，可以假设该参数符合某种条件（比如均值为0），带着这个假设进入到实际样本中。问这么一个问题：**<u>在已有假设的基础上，实际样本产生的概率有多大，是不是一个小概率事件？</u>**如果在此假设下，样本发生的概率极小，反过来极小概率的事件发生了，那很有可能是假设错了，我们完全可以拒绝原假设。因此，关键在于**<u>判断该样本产生是否为小概率事件</u>**——基于已知统计量构建分布，同时**<u>怎么定义小概率事件？</u>**到底概率多小才算小，这就引出了置信水平或显著性水平。

`两类错误`：

1. 第一类错误：小概率事件不是没有可能发生，假设定义发生概率为5%的事件为小概率事件，那也意味着在原假设成立的情况下，仍有5%的可能性会发生样本所看到的小概率事件，因此如果直接把原假设拒绝了，那就有5%的可能性犯错。**<u>原假设明明为正确的，但却把他拒绝了的概率，就是显著性水平</u>**
2. 第二类错误：**<u>原假设明明错了，但是根据样本计算的统计量却接受了它的概率</u>**。这个算起来就不容易了，首先就需要正确的统计量到底是什么，然后带到犯错的区间内去求概率。

`基本步骤`：

1. 根据实际情况提出原假设 $H_0$ 和备择假设 $H_1$。如果是双边的 $H_0$ 肯定是等于，$H_1$ 是不等于。但如果是单边的话刚开始设定 $H_0$ 为与题中相反的方向（题中说减小，这个地方就设定为大于等于），备择假设和题中所说相符，然后将 $H_0$ 转化为等于。
2. 假设 $H_0$ 成立，构造适当的统计量
3. 基于该统计量，给定置信水平 $\alpha$ 根据统计量的分布情况查表，确定拒绝域 $W$
4. 根据样本的观察值计算统计量的值，将其与拒绝域作比较并下结论。

`统计量的构建` 和上一章完全相同的四种情况，注意拒绝域的符号不要搞错。

