# 概率统计



## 第一章 随机事件与概率



### 概率的定义（三个性质）

<img src="C:\Users\16332\AppData\Roaming\Typora\typora-user-images\image-20230701094150291.png" alt="image-20230701094150291" style="zoom:50%;" />

### 全概率公式和贝叶斯公式

$$
P(B) = \sum P(A_i)P(B|A_i)
$$

$$
P(A|B) = \frac{P(A)P(B|A)}{P(B)}
$$

先验概率和后验概率：前者是在没有证据的情况下由以往的分析得到的，后者是有了一些证据对先验概率的修正（在有条件<证据>的情况下修正的）。

### 独立事件

`1. 定义`
$$
P(AB) = P(A)P(B) \\
P(A|B) = P(A) \\
P(B|A) = P(B)
$$
`2. 性质`

- 如果 A 和 B 独立，那么 A反、B反、A、B 的各种组合都独立 不要怀疑！
- 概率为 0 的时间以及概率为 1 的时间与任意一个事件均相互独立；
- A 和 B 相互独立，有：$P(A|B) = P(A|\overline B)$，结合$P(\overline A|B) = 1 - P(A|B)$ 就可以得到相应的变化
- 互斥不一定独立，独立不一定互斥。二者本质上没有任何联系，根据公式就可以看出，只要 A 和 B 的概率同时都大于零那独立和互斥不可能同时出现。
- 对立一定互斥，但互斥不一定对立

### 常见考点与经典例题

`1. 考点梳理`：牢记先构建事件集合再深入概率

- 有关随机事件集合的书写与概率的计算（本质是集合的计算，画出集合VN图）
- 古典概型排列组合，几何概型画图计算
- 有关概率的各种证明，尤其是条件概率的证明，是对贝叶斯公式的深入考察
- 全概率公式和贝叶斯公式
- 乘法公式计算概率
- 根据独立计算概率 => 伯努利古典概型

`2. 组合计算` 设 A，B 是两个随机事件，且 $P(A) = 0.6, P(B) = 0.7$，问 $P(AB)$ 何时最大何时最小？

```
P(AB) = P(A) + P(B) - P(A 并 B)
P(A 并 B) 最大时，即为 1 时，该值最小
P(A 并 B) 最小时，即 A 为 B 的子集时，该值最大
```



## 一维随机变量及其分布

### 分布函数的概念

引出概率的思路是从随机事件开始的，随机事件自然引申出了分布函数而不是概率密度函数，分布函数的==右连续性==一定要牢记，这样才能求离散分布的概率密度函数。

对于任意随机事件都有相对应的分布函数。离散型随机变量的分布函数花招很多一定要注意。所谓分布就是对事件发生概率的衡量，离散的也比较好理解，连续的一定要用积分的方式来思考。

### 离散型随机变量 —— 常见的的离散分布

> 概率所度量的就是发生 x 次这一个点上的可能性，十分直观

`1. 0 - 1 分布` 只有两种情况

`2. 二项分布` $B(n, p)$，进行 n 次，每次的概率为 p
$$
P(X = k) = C^k_n p^k(1-p)^{n-k}
$$
`3. 泊松分布` 参数是 $\lambda$
$$
P\{X = k\} = \frac{\lambda^k}{k!}e^{-\lambda}
$$
现实生活中的很多问题都符合泊松分布：地震、火山爆发、特大洪水、排队问题、电话呼唤次数。

注意，如果一个分布为二项分布，但是其 n 很大，难以计算概率时，可以将其近似看作泊松分布，$\lambda = np$ 这是完全可以证明的。泊松分布就是二项分布的极限。

`4.集合分布` 参数是 p，第 k 次为第一次出现的概率
$$
P\{X=k\} = (1-p)^{k-1}p
$$
`5.超几何分布` N 个产品中，其中有 M 是坏的，抽取 n 个，问抽到 k 个好的概率是多少？
$$
P\{X=k\} = \frac{C^k_{M-N}C^{n-k}_M}{C^n_N}
$$
离散型随机变量比较简单，只需要记住泊松分布即可

### 连续型随机变量

`1. 定义` 一旦进入连续型，就没有点的概念了，都是要概率对于一个点而言肯定是零，因此我们要考虑一个范围。对于连续型随机变量最重要的一个结果是说：P = 0 并不代表不发生，所以以下两个说法均不对：

- $P(AB) = 0$ 则 A，B 互斥
- 若 P(A) = P(AB)，则 A 是 B 的真子集

`2. 分布函数的性质` $f(x)$ 是某一个连续型随机变量 X 的概率密度函数的充要条件是 

- 非负性：$f(x) > 0$
- 归一性：$f(x)$ 从负无穷到正无穷的积分为 1

#### 常见的连续性随机变量

`1. 均匀分布` X ~ U(a, b) 

`2. 指数分布` X ~ E($\lambda$)，和泊松分布一起记忆
$$
f(x) = \lambda e^{-\lambda x}, x > 0
$$
`3. 正态分布` X ~ N(u, $\sigma^2$)
$$
f(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-u)^2}{2\sigma^2}}
$$

`4. Γ 函数与 Γ 分布`

<img src="C:\Users\16332\AppData\Roaming\Typora\typora-user-images\image-20230701124826745.png" alt="image-20230701124826745" style="zoom:50%;" />

<img src="C:\Users\16332\AppData\Roaming\Typora\typora-user-images\image-20230701124838699.png" alt="image-20230701124838699" style="zoom:56%;" />

#### 随机变量的分布转换

已知 X 的概率密度函数为 $f_X(x)$，且知道 $Y = G(X)$，问 Y 的概率密度函数，或者分布函数。如果是离散的，也需要慢慢梳理，连续的需要严格按照以下步骤进行：

`Step 1` 严格按照定义进行推导
$$
F_Y(y) = P\{Y \le y\} \\
=P\{G(X) \le y\} \\
=P\{\phi(y) \le X \le \gamma(y\}
$$
`Step 2` 积分求出 $F_Y(y)$

`Step 3` 求导得到 $f(y)$

### 常见考点及经典例题

`1.常见考点`

- 分布函数的基本概念：知随机变量（离散或连续）求解分布函数进而得到概率密度函数，知道分布函数的具体形式求解其中的参数；
- 应用常见的分布求解问题
- 函数的分布：分布函数的转化，已知随机 X 和其概率密度函数求解随机变量 $Y = G(X)$ 的概率密度函数（积分一定要准确，注意区分区间）。

`2. 分布函数的性质`

- 设随机变量 X 的密度函数为 $f(x)$，则下列函数中必为某随机变量的密度函数的是（纯套定义）
- 下面 $F(X)$ 可以为分布函数的是。（一定要把 F(x) 对应的 $f(x)$ 弄出来，然后就按照上面的定义即可—非负、标准、可加）

`3. 随机变量分布转换`

完全按照定义来求分布函数即可，不要试图走任何捷径。



## 二维随机变量及其分布

### 二维随机变量的含义

大多数现实场景中我们都不能只考虑其中一个随机变量，二是多维随机变量共同考虑，最简单的就是二维的情况，对两个变量进行同时考虑。

### 多维随机变量及其分布的定义

`1. 基本性质` 

- 分布函数必须要是非降的，随着 x 和 y 的增加必须值不能变小。

- 和一维完全相同，密度函数大于零，平面积分为 1，分布函数 x, y 趋近于正无穷为 1，趋近于负无穷为零。

- 不论对于 x 还是 y 都是右连续的

- $$
  P(x_1 < X < x_2, y_1 < Y < y_2) = \\
  F(x_2, y_2) - F(x_2, y_1) - F(x_1, y_2) + F(x_1, y_1) \ge 0
  $$

离散变量整体处理起来比较简单，就不再赘述，这一章关键在连续，在多元积分上，积分千万要会求。

<img src="C:\Users\16332\AppData\Roaming\Typora\typora-user-images\image-20230701114032033.png" alt="image-20230701114032033" style="zoom:33%;" />

`2. 连续随机变量的三大函数`

- 概率密度函数，按照定义就是分布的二元微分

- 边缘密度函数，直接从概率密度函数来：X 的边缘密度就对 Y 求积分，Y 的边缘密度就对 X 求积分；或者从分布函数来：求 X 的边缘分布就是让 Y 趋近于无穷大，求 Y 的边缘分布就是让 X 趋近于无穷大。

- 条件密度函数，就是概率密度函数除以边缘密度函数，Conditional 的那个变量是仍然存在的，此时变成了一个超参数。

  <img src="C:\Users\16332\AppData\Roaming\Typora\typora-user-images\image-20230701123228255.png" alt="image-20230701123228255" style="zoom:50%;" />

`3. 两个特殊的二维分布`

- 二维均匀分布：图形面积，十分简单

- 二维正态分布：五个参数 $u_1, u_2, \sigma^2_1, \sigma^2_2,\rho$（最后一个是 X 和 Y 的相关系数）
  $$
  f(x,y) = \frac{1}{\sqrt{2\pi}\sigma_1\sigma_2\sqrt{1-\rho^2}}e^{-\frac{1}{2(1-\rho^2)}[\frac{(x-\mu_1)^2}{\sigma_1^2} + \frac{(x-\mu_2)^2}{\sigma_2^2} - \frac{2\rho(x-\mu_1)(x-\mu_2)}{\sigma_1\sigma_2}]}
  $$
  参考一维正态分布记忆

### 随机变量的独立性

`1. 定义` 注意，定义中对独立性的定义是使用的分布函数，也就是说如果有：
$$
F(x, y) = F(x)F(y)
$$
那么随机变量  X 和 随机变量 Y 就是相互独立的，但是推导可以发现，对于连续函数而言，上述条件等价于：
$$
f(x,y) = f_X(x)f_Y(y)
$$
因此催生了一大批判断是否独立的题目，基本上来说和上面三大函数的求解是放在一起的。但是选择题中如果遇到，一定要用技巧：

- 先看定义域 X 和 Y 一定要不相关
- 再看 $f(x,y)$ 是不是 $g(x)h(y)$ 的形式

`2. 性质` 如果随机变量 X 和 Y 相互独立，且 $g(x), h(y)$ 是连续函数，那么由此得到的新随机变量 $g(X),h(Y)$ 相互独立。

### 二维随机变量的分布转换

`1. 定义`：给定 $Z = G(X)H(Y)$ 问 $Z$ 的分布怎么样。

`2. 方法`：

- 直接用积分方法，一定都要从分布函数出发！套定义先求 $F_Z(z)$ 再求 $f(z)$ 一定要学会分范围求解
- 公式法，有两个常见的 $Z$：
  - $Z = min\{X, Y\} => F_Z(z) = 1-[1-F_X(z)][1-F_Y(z)]$ 
  - $Z = max\{X, Y\} => F_Z(z) = F_X(z)F_Y(z)$
- 特殊性质，正态分布的可加性均值和方差直接相加即可（常数均值为其值，方差为 0）

### 常见考点与经典例题

`1. 常见考点`

- 联合分布函数和概率密度函数的定义与性质
- 求边缘分布函数（从边缘分布出发以及从概率密度出发）连续和离散的都要会
- 随机变量函数的分布

`2. 一个脑筋急转弯的证明，但也切实体现了基础` 设 $Y_1$\~$P(\lambda_1)$，$Y_2$\~$P(\lambda_2)$，且 $Y_1, Y_2$ 相互独立，证明 $Y_1 + Y_2$ \~ $P(\lambda_1+\lambda_2)$：

证明：因为 $P(Y_1 = m) = \frac{\lambda_1^m}{m!}e^{-\lambda_1}$，$P(Y_2 = n) = \frac{\lambda_2^n}{n!}e^{-\lambda_2}$，所以：
$$
P(Y_1 + Y_2 = k) = \sum_{i=0}^k P(Y_1=i)P(Y_2=k-i)
$$
然后进行继续推导，如果反过来看得时候还是不会，看符玺书 P53



## 数字特征

> 均值、方差、协方差 => 相关系数、矩

### 数学期望（均值）

`1. 定义` 每一个可能值 * 其对应的概率：

- 对于离散就是相乘加和
- 对于连续就是积分，函数变化后，就直接对目标函数进行积分即可，比如 $E(X^2),E(sin(X))$，对于依托于二维随机变量的变量可以通过二维积分进行求解。

`2. 性质`：

- 设 C 是一个常数，$E(C) = C$
- 设 X 是一个随机变量，C 是一个常数，$E(CX) = CE(X)$
- 线性可加，设 X，Y 是两个随机变量，则有：$E(aX\pm Y) = aE(X)\pm E(Y)$，再加亦可
- 若随机变量 X，Y 相互独立，则 $E(XY) = E(X)E(Y)$，再加亦可**反之则不然！！！！**只能说明线性不相关罢了

### 方差

`1. 定义` 偏离均值的程度：
$$
D(X) = E\{[X - E(X)]^2\} \\
D(X) = E(X^2) -E(X)^2
$$
因此说白了，还是和求均值一样，只不过 $E(X^2)$ 多了个平方而已

`2. 性质`：

- 设 C 是常数，则 $D(C) = 0$，因此如果 $D(X) = 0$，那么就一定有 $P\{X = E(X)\} = 1$  也就是说，X 始终都是一个值

- 设 X 是一个随机变量，a、b 是常数，那么就有 $D(aX + b) = a^2D(X)$

- 设 X，Y 是两个随机变量，则有：
  $$
  D(X\pm Y) = D(X) + D(Y) \pm 2Cov(X, Y)
  $$
  其中 $Cov(X, Y) = E(XY) - E(X)E(Y)$

- 若 $X, Y$ 相互独立，则 $D(X\pm Y) =D(X) \pm D(Y)$ 反之则不然，还是只能说明线性不相关。

### 协方差

`1. 定义` 衡量多个变量之间的线性相关关系：
$$
Cov(X, Y) = E(XY) - E(X)E(Y)
$$
这个的计算仍然是类似地，无非是在多元概率密度函数中求值罢了，注意如果是二元变量的话，求解 $E(X)$ 或 $E(Y)$ 有两种方法，但是为了简单起见，就直接在原函数上进行积分即可。

`2. 性质`

- $Cov(X, X) = D(X)$

- $Cov(X, Y) =Cov(Y, X)$

- $C$ 是一个常数，那么 $Cov(X, C) = 0$

- $a,b$ 为常数时，$Cov(aX,bY) = abConv$

- 完全线性可加：
  $$
  Cov(a_1X_1 + a_2X_2, b_1Y_1+a_2Y_2) = \\ a_1b_1Cov(X_1, Y_1) + a_1b_2Cov(X_1,Y_2) + a_2b_1Cov(X_2,Y_1) + a_2b_2Cov(X_2,Y_2)
  $$

- 三项情况：很重要！
  $$
  D(X + Y + Z) = D(X) + D(Y) + D(Z) +2Cov(X, Y) + 2Cov(X, Z) + 2Cov(Y, Z)
  $$

### （线性）相关系数

`1. 定义` 当时学习时可能理解得还没有那么深刻，后面来看，所谓相关系数在此处仅仅代表线性相关系数，只衡量两个变量之间的线性相关关系，定义公式如下：
$$
\rho_{XY} = \frac{Cov(X,Y)}{\sqrt{D(X)}\sqrt{D(Y)}}
$$
`2. 性质`

- 相关系数的绝对值小于等于 1 ，也就是说其值在 -1 和 1 之间，如果值等于 0 说明无线性相关性。还是要注意，这只是线性不相关，而非独立！！！

- 一个很重要的性质，经常考察：相关系数为 1 的意思是，二者的关系完全线性
  $$
  P\{Y = aX + b\} = 1 (a\ne0)
  $$

- 再次强调：一般地，X 和 Y 相互独立一定有 X 和 Y 不相关，但不相关只是线性不相关而不是独立

`3. 常考点`

- 很容易验证不相关，但一般来说验证独立很困难（参照上一章所学方法，需要求出两个变量相乘取得的新变量，这个过程极为困难）
- 因此如果出到这类的题目，一般来说答案都只会是，不相关而非独立！验证独立只需要求个二次相关或者三次相关！

### 矩

`1. 定义` 矩就是对所有数字特征的特征化表示，上述三大数字特征均可用矩来表征。

`2. 例子`

- k 阶原点矩：$E(X^k)$，当 $k=1$ 时表示均值。
- k 阶中心距：$E([X-E(X)]^k)$， 当 $k=2$ 时表示方差。
- k + l 阶混合矩：$E(X^kY^l) = E([X-E(X)]^k[Y-E(Y)]^l)$，当 $k=l=1$ 时表示协方差。

### Summary

| 分布类型                              | 均值                     | 方差                     |
| ------------------------------------- | ------------------------ | ------------------------ |
| 0-1 分布：$B(1, p)$                   | $p$                      | $p(1-p)$                 |
| 二项分布：$B(n, p)$                   | $np$                     | $np(1-p)$                |
| 泊松分布：$P(\lambda)$                | $\lambda$                | $\lambda$                |
| 几何分布：$P\{X = k\} = (1-p)^{k-1}p$ | $\frac{1}{p}$            | $\frac{1-p}{p^2}$        |
| Γ 分布（利用 Γ 函数进行求解）         | $\frac{\alpha}{\beta^2}$ | $\frac{\alpha}{\beta^2}$ |
| 均匀分布：$U(a,b)$                    | $\frac{a+b}{2}$          | $\frac{(b-a)^2}{12}$     |
| 指数分布：$E(\lambda)$                | $\frac{1}{\lambda}$      | $\frac{1}{\lambda^2}$    |
| 正态分布：$N(\mu, \sigma)$            | $\mu$                    | $\sigma$                 |

### 常见考点与相关习题

- 求随机变量函数的数学期望（没办法，只能套公式）
- 求符合变量函数的数学期望（min(X, Y)，max(X, Y)等）（就是设 U=G(x,y) 求解得到 U 的概率密度函数，然后得到期望值）
- 计算方差协方差相关系数并基于此判断是否相关 or 独立（一般独立很难验证出来，都是不相关）
- 有关正态分布的相关概念：不独立的正态分布组合的二元分布可能不是多元正态分布

<img src="C:\Users\16332\AppData\Roaming\Typora\typora-user-images\image-20230701142617173.png" alt="image-20230701142617173" style="zoom:50%;" />



## 大数定律和中心极限定理

### 大数定律

`定义`：当实验次数足够多的时候，整体实验的均值就会趋近于一个定值

#### 切比雪夫不等式

设随机变量的数学期望与方差都存在，则对任意的无穷小都有：
$$
P\{|X - \mu| \ge \epsilon\} \le \frac{\sigma^2}{\epsilon^2} \\
P\{|X- \mu| \le \epsilon \} \ge 1- \frac{\sigma^2}{\epsilon^2}
$$

切比雪夫不等式可以实现：大致估计某一范围内的概率。

#### 依概率收敛

$$
lim_{n\to\infin}\{|Y_n - a| < \epsilon\} = 1
$$

就说序列 $Y_i$ 依概率收敛至 $a$，对于单个变量没有任何意义，但是在大数定律中却可以得到充分有效的使用。注意和趋近于 a 的区别。

#### 三个大数定律

大数定律是描述大量现象平均结果稳定性的定理。

`1. 最 general 的` 切比雪夫大数定律：如果 $X_1, X_2, \cdots, X_n,\cdots$ 是相互独立的随机变量序列，并且每个 $X_i$ 均有有限的数学期望 $E(X_i)$ 和有限的方差 $D(X_i)$，则有：
$$
lim_{n\to\infin}P\{|\frac{1}{n}\sum_{i=1}^n(X_i) - \frac{1}{n}\sum_{i=1}^nE(X_i)|\le\epsilon\} = 1
$$
也就意味着，只要能够给出无穷多个分布的均值，就可以求解前面序列的依概率收敛结果，但是这个在现实中不太好用。

`2. 加一个同分布的` 辛钦大数定律：如果上述相互独立的随机变量同分布，且均值 $E(X_i) = \mu$ 那么就有：
$$
lim_{n\to\infin}P\{|\frac{1}{n}\sum_{i=1}^n(X_i) - \mu|\le\epsilon\} = 1
$$
借助此，只要给出该分布的均值，即可完成对序列依概率收敛值的求解。由此还有一个推论，如果 $E(X_i^k) = \mu_k$ 那么就有
$$
lim_{n\to\infin}P\{|\frac{1}{n}\sum_{i=1}^n(X_i^k) - \mu_k|\le\epsilon\} = 1
$$
这个是常考的点，比如问 $\frac{1}{n}\sum_{i=1}^n(X_i^2)$ 依概率收敛于多少，就要求$E(X^2) = D(X) + E(X)^2$ 

`3. 一个特殊情形` 伯努利大数定律：简单的随机抽样情况，设 $n_A$ 为在 $n$ 次独立实验中事件 $A$ 发生的频数，$p$ 是事件 A 在每次实验中发生的概率，则有：
$$
lim_{n\to\infin} P\{|\frac{n_A}{n} - p| < \epsilon\} = 1
$$
也就是说：随着实验次数的增加，事件发生的频率越来越接近于事件发生的概率。

### 中心极限定理（实际应用必考点）

`定义` 数字逐渐变大，特征逐渐逼近分布中心 —— 正态分布

#### 独立同分布中心极限定理（列维-林德伯格定理）

设随机变量 $X_1,X_2,\cdots,X_n,\cdots$ 独立同分布，且均值为 $\mu$ 方差为 $\sigma^2$，则有：
$$
\sum_{i=1}^n X_i 近似服从 N(n\mu,n\sigma^2)
$$

#### 二项分布以正态分布为极限分布的中心极限定理（棣莫弗-拉普拉斯定理）

二项分布本就是多次 $0-1$ 分布聚集，因此这算是对上面定理的一个**特殊化推广**：设随机变量 $X$ 服从 $B(n, p)$，则当 $n$ 充分大的时候，$X$ 近似服从 $N(np, np(1-p))$

#### 必考题目

给一个实际应用场景，可以是 n 个独立同分布给均值和方差，也可以就是一个二项分布，比如要想让 100 车床正常工作的概率大于 p 至少需要多少电？每张车床工作独立，正常工作概率为 0.6

`Step 1 由基础分布转化为正态分布：`

设同时工作的车床为 X，X 服从 B(100, 0.6)，进而转化为 N(60, 48)

`Step 2 识别题目含义列出准确算式：`
$$
P\{T > X\} > p \\
P\{\frac{X - \mu}{\sigma} < \frac{T-\mu}{\sigma}\} > p \\
P\{\frac{X - 60}{\sqrt{48}} < \frac{T-60}{\sqrt{48}}\} > p \\ 
\Phi(\frac{T-60}{\sqrt{48}}) > p
$$
`Step 3 根据题目给出的分位点，精准求解：` 

一般来说 p 都会以分位点的形式给出，这样只需要求解一个简单的二次或者一次方程即可。



## 数理统计的基本概念（进入统计）

### 总体、样本以及样本统计量

#### 总体的定义和性质

`定义`：之前我们的讨论始终围绕着总体 $X$ 的分布进行，但在实在的世界中我们很难看到总体。所以我们在聊统计时，无非聊两个问题：**<u>理论推导得出</u>**总体服从某一分布，可以得到样本相关数字特征；看到样本服从某一特性推测总体服从某一分布，或者样本是否显著。这就是总体和样本之间的关系。

`性质`

1. 独立性：从总体中抽取的样本之间相互独立
2. 同分布：抽取的样本之间同分布
3. 表示方法：从总体 $X$ 中抽取得到的样本记作 $X_1,X_2\cdots,X_n$ 这些样本的实际观测值记为 $x_1, x_2, \cdots, x_n$

#### 统计量

`定义`：能够用总体已知（绝不可能是未知）参数表征的样本数字特征，都是统计量，样本每次抽取都不一样，统计量也各有不同。

`常见统计量（重点！）`

1. 样本的均值：$\overline{X} = \frac{1}{n}\sum_{i=1}^n(X_i)$
2. 样本的方差：$S^2 = \frac{1}{n-1}\sum_{i=1}^n(X_i-\overline{X})^2$ 这里是 $n-1$ 而不是 $n$ 的原因我们已经深刻了解到了自由度和含义。
3. 当然和总体数字特征一样，也会拥有标准差、原点矩（-0）、中心矩（-均值）但都不重要

`最重要的特征！！！！`	设总体 $X$ 的均值为 $E(X)$ 方差为 $D(X)$

1. 样本均值的期望：$E(\overline{X}) = E(X)$
2. 样本均值的方差：$D(\overline{X}) = \frac{1}{n}D(X)$
3. 样本方差的期望：$E(S^2) = D(X)$ 保证了无偏性
4. 样本方差的方差：$D(S^2)$ 可用卡方分布求解

### 各种由正态分布组合的出来的新分布

#### 卡方分布

`定义`：来自<u>**正态分布**</u>总体的n个随机变量**<u>平方的和</u>**，就是一个服从**<u>自由度为 n</u>**的卡方分布。

`性质`

1. 自由度为 n 的卡方分布的均值为 n，方差为 2n（数字特征，也很好证明，卡方分布本质是一种 Γ 分布）
2. 自由度分别为 n1、n2 的独立卡方分布加和服从自由度为 n1+n2 的卡方分布（用以求解分布中的未知参数，进而使得某分布服从卡方分布）

#### t 分布（单变量检验）

`定义`：由两个独立的分布 X，Y 组合得到，其中 X~N(0, 1)，Y~自由度为 n 的卡方分布，则 $\frac{X}{\sqrt{Y/n}}$ 服从自由度为 n 的 t 分布（谨记！）。

#### F 分布（多变量检验）

`定义`：由两个相互独立的卡方分布，一个为 X\~自由度(n1)的卡方分布，一个为Y\~ 自由度(n2)的卡方分布，组合得到 $F = \frac{X/n1}{Y/n2}$ 服从 F(n1, n2)。

`性质`：和 t 分布互动一下，若 T\~t(n) 则 $T^2$~F(1,n)

#### 分位点（查表）

只要记住四种分布的概率分布图，就明白分位点的含义了：

1. 正态分布、t 分布是关于 y 轴对称的；
2. 卡方和 F 分布所有值都大于 0。

### 正态分布总体下的样本分布（后续很多理论知识的基础）

先看单：假设总体 $X$\~$N(\mu,\sigma^2)$，$(X_1, X_2, \cdots, X_n)$ 为来自总体 $X$ 的简单随机样本则：

1. $\overline{X}$ 服从 $N(\mu, \frac{\sigma^2}{n})$

2. $\frac{(n-1)S^2}{\sigma^2}$ 服从自由度为`n-1​的卡方分布`。（这个定理的详细推导过程并不好做，但是直观能感受出来，借助该结论可以求解很多结论！）

3. 一个 t 分布（这是计量中做 t 检验的基础理论！）
   $$
   T = \frac{\overline{X} - \mu}{S/\sqrt{n}}
   $$
   服从自由度为 $n-1$ 的 t 分布，这个十分重要！

正是因为有了这些基础，就可以在总体和个体之间搭建分布进行详细计算了。

再看双：假设总体 $X$\~$N(\mu_1, \sigma_1^2)$ ，$Y$\~$N(\mu_2, \sigma_2^2)$ 从两总体中进行抽样，得到抽样结果，仍然可以构造相关统计量：

1. $$
   T = \frac{(\overline{X} - \mu_1) - (\overline{Y} - \mu_2)}{S_w\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} 服从 t(n_1 + n_2 -2)
   $$

   其中：
   $$
   S_w =\sqrt{\frac{(n_1-1)S_1^2 + (n_2-1)S_2^2}{n_1 + n_2 -2}}
   $$

2. $$
   F = \frac{S_1^2/\sigma_1^2}{S_2^2/\sigma^2_2} 服从 F(n_1-1, n_2-1)
   $$




## 参数估计 —— 由样本估计总体参数

> 知道总体大概呈什么分布情况，但是其中有些参数仍未知，所以需要根据所观察到得样本数据进行估计，或者类似于计量经济学对总体函数关系参数做估计。
>
> 注意：估计出来的参数，仍然是一个变量，仍然有均值方差

### 点估计

`定义`：最简单的估计方法，在样本上随便设定一个统计量（估计量），然后根据样本的估计值计算该统计量的值，令该值等于总体理论推导值，进而得出总体中未知变量

#### 矩估计 （采用【矩】这一统计量进行估计）

`理论`

1. 在总体上有：一阶原点矩$E(X)$，二阶原点矩$E(X^2)$，二阶中心矩$D(X)$
2. 在样本上有：一阶原点矩$\overline{X}$，二阶原点矩$\frac{1}{n}\sum_{i=1}^nX^2_i$，二阶中心矩$\frac{1}{n}\sum_{i=1}^n(X_i-\overline{X})^2$

由中心极限定理可得，当 n 趋近于无穷大时，样本矩和总体矩相等，进而反解出目标变量。

`应用`

1. 第一步先看有几个未知参数：如果有 1 个，优先使用低阶矩，如果有 2 个，一阶矩和二阶矩都要用上，二阶矩建议使用二阶中心矩；
2. 先在总体上构建矩估计方程，得到参数如何用总体特征表示；
3. 再在样本上估计出数字特征，反代回 2 得到估计值。

#### 极大似然估计（让看到的样本发生的概率最大化）

`理论`：对于带有未知参数的一个总体分布而言，观测到了一个确定的样本，让该样本发生概率最大的那个参数，才是好参数。

`应用` （尤其注意均匀分布 max 和 min 的情况）

1. Step 1：写出似然函数（离散连续两种情况）

   <img src="C:\Users\16332\AppData\Roaming\Typora\typora-user-images\image-20230430145933148.png" alt="image-20230430145933148" style="zoom:33%;" />

2. Step 2：写出对数

   <img src="C:\Users\16332\AppData\Roaming\Typora\typora-user-images\image-20230430145959587.png" alt="image-20230430145959587" style="zoom:33%;" />

3. Step 3：求解参数

   <img src="C:\Users\16332\AppData\Roaming\Typora\typora-user-images\image-20230430150026697.png" alt="image-20230430150026697" style="zoom:33%;" />

4. Step 4：因为上面仅仅是观测值，不同的样本可能有不同的观测值，因此要把观测值$x_i$换为样本值$X_i$

`最大似然估计的不变性` 就是说如果 $\hat{\theta}$ 是参数 $\theta$ 的最大似然估计值，那么$g(\hat{\theta})$ 也是$g(\theta)$ 的最大似然估计值。也就是说：求解出来估计结果，可以进行函数转换。

#### 点估计选择估计出来的结果的标准

`目的`：两种点估计方法，估计出来的最终结果可能有所不同，到底该怎么选呢？提供了许多的标准。

`无偏性`：$E(\hat{\theta}) = \theta$ 则无偏，最明显的就是说，样本的二阶中心矩并不是无偏的，但样本方差是无偏的。其实无非就是对估计出的参数 $Y$ 进一步求期望，如果 $Y = \overline{X}$ 那就很简单了，但是如果 $Y = max(X_1, X_2,\cdots,X_n)$ 这就麻烦了，需要先把 $Y$ 的分布给出来<可以用公式！>然后再求均值 or 方差

`有效性`：**<u>只有无偏估计量才有资格比较有效性</u>**，两个无偏估计量谁的方差越小谁越有效，因此这个地方就和无偏性对应起来了，上面讲的是均值，此处讲的是方差。求方差的那一套拿过来直接用！

`一致性`：有效性暗含着一个趋势，是指随着样本容量越来越大，样本中所含的分布信息越来越多，估计出的参数越来越接近总体参数。

### 区间估计（一个硬币的两面）

`定义`：给定样本所得到的点估计虽然表面上只是一个确定的值，但实际上其再一个分布中（要不怎么会有均值方差呢，求无偏有效性呢？）我们当然可以根据这个分布来判断估计值可能所在的区间。

`方法（在此我们只考虑正态分布总体）` $N(\mu, \sigma^2)$，凡是不为该分布的，都通过中心极限定理，转变为该分布。

思路其实都是完全相同的：先找一个无偏估计量，然后将此无偏估计量和待估计参数结合起来构造一个常见的分布，然后反解即可。

1. 求 $\mu$ 的区间估计（总体的 $\mu$ 未知，但 $\sigma^2$ 已知）构建**<u>正态分布统计量</u>**

   <img src="C:\Users\16332\AppData\Roaming\Typora\typora-user-images\image-20230430154523267.png" alt="image-20230430154523267" style="zoom:19%;" />

   <img src="C:\Users\16332\AppData\Roaming\Typora\typora-user-images\image-20230430154808491.png" alt="image-20230430154808491" style="zoom:20%;" />

2. 求 $\mu$ 的区间估计（总体的 $\mu,\sigma^2$ 均未知）构建**<u>t 统计量</u>**

   <img src="C:\Users\16332\AppData\Roaming\Typora\typora-user-images\image-20230430155401302.png" alt="image-20230430155401302" style="zoom:33%;" />

   这就是我们在做回归估计参数时，惯用的检验显著与否的手段。

3. 求 $\sigma^2$ 的区间估计（总体的 $\mu,\sigma^2$ 均未知）

   <img src="C:\Users\16332\AppData\Roaming\Typora\typora-user-images\image-20230430161226935.png" alt="image-20230430161226935" style="zoom:30%;" />

   <img src="C:\Users\16332\AppData\Roaming\Typora\typora-user-images\image-20230430161246646.png" alt="image-20230430161246646" style="zoom:25%;" />

4. 求 $\sigma^2$ 的区间估计（总体的 $\mu$ 已知，但 $\sigma^2$ 未知）

   将上面卡方分布的分布改为 $\sum_{i=1}^{n}(x_i-\mu)^2$ 这样可以减少自由度的损失，更加精准。

当然，以上结论都是给的双侧，但是单侧的原理和上述完全相同，只要记住如何构造分布，就什么都不怕！



## 假设检验

> 这和上面一章参数估计是一脉相承的，上一章要求估计出某个参数的准确值。现在我们要根据样本情况，验证某个参数是否满足某些条件，在假设成立的前提下，小概率事件是否发生。

`定义`：从小概率事件是否发生的角度来讲这个故事：不论是参数估计还是假设检验，我们针对的始终是总体的参数。在不知道总体的某个参数时，可以假设该参数符合某种条件（比如均值为0），带着这个假设进入到实际样本中。问这么一个问题：**<u>在已有假设的基础上，实际样本产生的概率有多大，是不是一个小概率事件？</u>**如果在此假设下，样本发生的概率极小，反过来极小概率的事件发生了，那很有可能是假设错了，我们完全可以拒绝原假设。因此，关键在于**<u>判断该样本产生是否为小概率事件</u>**——基于已知统计量构建分布，同时**<u>怎么定义小概率事件？</u>**到底概率多小才算小，这就引出了置信水平或显著性水平。

`两类错误`：

1. 第一类错误：小概率事件不是没有可能发生，假设定义发生概率为5%的事件为小概率事件，那也意味着在原假设成立的情况下，仍有5%的可能性会发生样本所看到的小概率事件，因此如果直接把原假设拒绝了，那就有5%的可能性犯错。**<u>原假设明明为正确的，但却把他拒绝了的概率，就是显著性水平</u>**
2. 第二类错误：**<u>原假设明明错了，但是根据样本计算的统计量却接受了它的概率</u>**。这个算起来就不容易了，首先就需要正确的统计量到底是什么，然后带到犯错的区间内去求概率。

`基本步骤`：

1. 根据实际情况提出原假设 $H_0$ 和备择假设 $H_1$。如果是双边的 $H_0$ 肯定是等于，$H_1$ 是不等于。但如果是单边的话刚开始设定 $H_0$ 为与题中相反的方向（题中说减小，这个地方就设定为大于等于），备择假设和题中所说相符，然后将 $H_0$ 转化为等于。
2. 假设 $H_0$ 成立，构造适当的统计量
3. 基于该统计量，给定置信水平 $\alpha$ 根据统计量的分布情况查表，确定拒绝域 $W$
4. 根据样本的观察值计算统计量的值，将其与拒绝域作比较并下结论。

`统计量的构建` 和上一章完全相同的四种情况，注意拒绝域的符号不要搞错。

